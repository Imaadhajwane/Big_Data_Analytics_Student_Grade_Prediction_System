{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Importing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, mean, stddev, corr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Student Performance Analysis\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Machine Learning Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Set up of Pyspark and Load of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = spark.read.csv(r'C:\\Users\\iamim\\OneDrive\\Desktop\\Code\\dataset.csv', header=True, inferSchema=True)\n",
    "# Show first 10 rows in PySpark\n",
    "# Convert PySpark DataFrame to Pandas DataFrame (limit to 10 rows)\n",
    "pandas_df = df.limit(10).toPandas()\n",
    "# Use Pandas style to apply background gradient and improve table appearance\n",
    "styled_table = pandas_df.style.background_gradient(cmap=\"Purples_r\") \\\n",
    "                                .set_table_styles([{'selector': 'thead th', 'props': [('background-color', '#6a0dad'), ('color', 'white')]}]) \\\n",
    "                                .format({'time_spent_hours': '{:.2f}', \n",
    "                                         'assignments_completed': '{:.0f}', \n",
    "                                         'quiz_scores': '{:.2f}', \n",
    "                                         'previous_gpa': '{:.2f}', \n",
    "                                         'attendance_rate': '{:.2f}'})\n",
    "# Display the styled table (works in Jupyter notebooks)\n",
    "styled_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get Data Frame Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows\n",
    "row_count = df.count()\n",
    "\n",
    "# Number of columns\n",
    "column_count = len(df.columns)\n",
    "\n",
    "print(f\"Shape of the DataFrame: Rows = {row_count}, Columns = {column_count}\")code.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of each column\n",
    "df.printSchema()\n",
    "\n",
    "# Step 1: Data Types of each column\n",
    "# Get the schema of the DataFrame (data types)\n",
    "schema_data = [(field.name, field.dataType) for field in df.schema.fields]\n",
    "\n",
    "# Convert schema information to a Pandas DataFrame for better formatting\n",
    "schema_df = pd.DataFrame(schema_data, columns=['Column Name', 'Data Type'])\n",
    "\n",
    "# Display the schema with Pandas styling\n",
    "schema_styled = schema_df.style.background_gradient(cmap=\"Blues\") \\\n",
    "                                .set_table_styles([{'selector': 'thead th', 'props': [('background-color', '#003366'), ('color', 'white')]}]) \\\n",
    "                                .set_properties(**{'text-align': 'center'}) \\\n",
    "                                .set_caption(\"Data Types of Each Column\")\n",
    "\n",
    "# Display the styled schema table (use in Jupyter notebook or equivalent)\n",
    "schema_styled\n",
    "\n",
    "# Step 2: Check for distinct values in each column (similar to checking for nulls)\n",
    "distinct_values = df.select([countDistinct(col(c)).alias(c) for c in df.columns]).collect()\n",
    "\n",
    "# Convert distinct values to a Pandas DataFrame for better formatting\n",
    "distinct_values_df = pd.DataFrame(distinct_values[0].asDict().items(), columns=['Column Name', 'Distinct Count'])\n",
    "\n",
    "# Display distinct values with Pandas styling\n",
    "distinct_values_styled = distinct_values_df.style.background_gradient(cmap=\"Purples_r\") \\\n",
    "                                            .set_table_styles([{'selector': 'thead th', 'props': [('background-color', '#6a0dad'), ('color', 'white')]}]) \\\n",
    "                                            .set_properties(**{'text-align': 'center'}) \\\n",
    "                                            .set_caption(\"Distinct Values Count for Each Column\")\n",
    "\n",
    "# Display the styled distinct values table (use in Jupyter notebook or equivalent)\n",
    "distinct_values_styled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run describe() in PySpark\n",
    "describe_df = df.describe()\n",
    "\n",
    "# Step 2: Convert PySpark DataFrame to Pandas for better formatting\n",
    "describe_pandas_df = describe_df.toPandas()\n",
    "\n",
    "# Step 3: Use Pandas style to improve the table appearance\n",
    "styled_describe = describe_pandas_df.style.background_gradient(cmap=\"Blues\") \\\n",
    "                                      .set_table_styles([{'selector': 'thead th', \n",
    "                                                          'props': [('background-color', '#003366'), \n",
    "                                                                    ('color', 'white')]}]) \\\n",
    "                                      .set_properties(**{'text-align': 'center'}) \\\n",
    "                                      .set_caption(\"Statistical Summary of Numerical Columns\")\n",
    "\n",
    "# Step 4: Display the styled table (works in Jupyter notebook)\n",
    "styled_describe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "numeric_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "\n",
    "# Get the number of numerical columns\n",
    "num_numerical_columns = len(numeric_columns)\n",
    "\n",
    "# Show the result\n",
    "print(f\"Number of numerical columns: {num_numerical_columns}\")\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "numeric_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "\n",
    "# Print the names of the numerical columns\n",
    "print(\"Numerical columns:\", numeric_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric columns\n",
    "numeric_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n",
    "# Initialize an empty dictionary to store correlations\n",
    "correlation_data = {col: [] for col in numeric_cols}\n",
    "\n",
    "# Compute correlation between each pair of numeric columns\n",
    "for col1 in numeric_cols:\n",
    "    for col2 in numeric_cols:\n",
    "        if col1 != col2:\n",
    "            corr_value = df.corr(col1, col2)\n",
    "        else:\n",
    "            corr_value = 1.0  # Correlation with itself is always 1\n",
    "        correlation_data[col1].append(corr_value)\n",
    "\n",
    "# Convert the dictionary to a Pandas DataFrame for better formatting\n",
    "correlation_df = pd.DataFrame(correlation_data, index=numeric_cols)\n",
    "\n",
    "# Style the correlation matrix\n",
    "styled_corr = correlation_df.style.background_gradient(cmap=\"coolwarm\") \\\n",
    "                                .set_table_styles([{'selector': 'thead th', \n",
    "                                                    'props': [('background-color', '#003366'), \n",
    "                                                              ('color', 'white')]}]) \\\n",
    "                                .set_properties(**{'text-align': 'center'}) \\\n",
    "                                .set_caption(\"Correlation Matrix of Numeric Columns\")\n",
    "\n",
    "# Display the styled correlation matrix (works in Jupyter notebook)\n",
    "styled_corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas for visualization\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Create a correlation heatmap using seaborn\n",
    "plt.figure(figsize=(10, 10))  # Increase figure size for better clarity\n",
    "correlation_matrix = pandas_df[numeric_cols].corr()\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            fmt=\".2f\",  # Format the annotations to two decimal places\n",
    "            cmap='RdYlGn', \n",
    "            linewidths=0.5, \n",
    "            linecolor='grey',  # Color for grid lines\n",
    "            cbar_kws={\"shrink\": .8},  # Shrink color bar\n",
    "            square=True,  # Make cells square-shaped\n",
    "            vmin=-1, vmax=1)  # Set limits for color mapping\n",
    "\n",
    "# Enhance the plot with title and labels\n",
    "plt.title(\"Correlation Heatmap\", fontsize=18, fontweight='bold', pad=20)  # Title with padding\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels\n",
    "plt.yticks(rotation=0, fontsize=12)  # Rotate y-axis labels\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of tick-labels\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Categorical Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, BooleanType\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "categorical_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, (StringType, BooleanType))]\n",
    "\n",
    "# Print the names of the categorical columns\n",
    "print(\"Categorical columns:\", categorical_columns)\n",
    "\n",
    "# Print the count of categorical columns\n",
    "print(\"Number of categorical columns:\", len(categorical_columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Frequency Distribution of Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get frequency counts for categorical columns\n",
    "categorical_cols = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']\n",
    "\n",
    "# Create a dictionary to store the styled DataFrames\n",
    "styled_freq_dfs = {}\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    # Calculate frequency counts and convert to Pandas DataFrame\n",
    "    freq_df = df.groupBy(col_name).count().orderBy('count', ascending=False).toPandas()\n",
    "    \n",
    "    # Style the DataFrame\n",
    "    styled_freq = freq_df.style.background_gradient(cmap=\"RdYlGn\") \\\n",
    "                                 .set_table_styles([{'selector': 'thead th', \n",
    "                                                     'props': [('background-color', '#4B0082'), \n",
    "                                                               ('color', 'white')]}]) \\\n",
    "                                 .set_properties(**{'text-align': 'center'}) \\\n",
    "                                 .set_caption(f\"Frequency Counts for {col_name}\")\n",
    "    \n",
    "    # Store the styled DataFrame in the dictionary\n",
    "    styled_freq_dfs[col_name] = styled_freq\n",
    "\n",
    "    # Display the styled DataFrame\n",
    "    print(f\"Frequency Counts for {col_name}:\")\n",
    "    display(styled_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total row count\n",
    "row_count = df.count()\n",
    "\n",
    "# Calculate percentage of missing values for each column\n",
    "missing_percentage = df.select([\n",
    "    (F.when(F.count(col(c)) == 0, 1).otherwise(0) * 100).alias(f'{c}_missing_percentage') \n",
    "     for c in df.columns\n",
    "])\n",
    "\n",
    "# Collect the results\n",
    "missing_percentage_values = missing_percentage.collect()[0].asDict()\n",
    "\n",
    "# Convert to Pandas DataFrame for better formatting\n",
    "missing_percentage_df = pd.DataFrame(list(missing_percentage_values.items()), columns=['Column Name', 'Missing Percentage'])\n",
    "\n",
    "# Style the missing value percentage table\n",
    "styled_missing = missing_percentage_df.style.background_gradient(cmap=\"RdYlGn\") \\\n",
    "                                        .set_table_styles([{'selector': 'thead th', \n",
    "                                                            'props': [('background-color', '#b22222'), \n",
    "                                                                      ('color', 'white')]}]) \\\n",
    "                                        .set_properties(**{'text-align': 'center'}) \\\n",
    "                                        .set_caption(\"Percentage of Missing Values for Each Column\")\n",
    "\n",
    "# Display the styled missing value percentage table (works in Jupyter notebook)\n",
    "styled_missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Variance and Standard Deviation of Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate standard deviation for each numeric column\n",
    "stddev_df = df.select([F.stddev(col(c)).alias(f'{c}_stddev') for c in numeric_cols])\n",
    "\n",
    "# Step 2: Calculate variance for each numeric column\n",
    "variance_df = df.select([F.variance(col(c)).alias(f'{c}_variance') for c in numeric_cols])\n",
    "\n",
    "# Step 3: Collect the results into Pandas DataFrames\n",
    "stddev_values = stddev_df.collect()[0].asDict()\n",
    "variance_values = variance_df.collect()[0].asDict()\n",
    "\n",
    "stddev_pandas_df = pd.DataFrame(list(stddev_values.items()), columns=['Column Name', 'Standard Deviation'])\n",
    "variance_pandas_df = pd.DataFrame(list(variance_values.items()), columns=['Column Name', 'Variance'])\n",
    "\n",
    "# Step 4: Style the standard deviation and variance tables\n",
    "styled_stddev = stddev_pandas_df.style.background_gradient(cmap=\"RdYlGn_r\") \\\n",
    "                                    .set_table_styles([{'selector': 'thead th', \n",
    "                                                        'props': [('background-color', '#4B0082'), \n",
    "                                                                  ('color', 'white')]}]) \\\n",
    "                                    .set_properties(**{'text-align': 'center'}) \\\n",
    "                                    .set_caption(\"Standard Deviation for Numeric Columns\")\n",
    "\n",
    "styled_variance = variance_pandas_df.style.background_gradient(cmap=\"RdYlGn\") \\\n",
    "                                      .set_table_styles([{'selector': 'thead th', \n",
    "                                                          'props': [('background-color', '#228B22'), \n",
    "                                                                    ('color', 'white')]}]) \\\n",
    "                                      .set_properties(**{'text-align': 'center'}) \\\n",
    "                                      .set_caption(\"Variance for Numeric Columns\")\n",
    "\n",
    "# Step 5: Display the styled tables (works in Jupyter notebook)\n",
    "display(styled_stddev)\n",
    "display(styled_variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate skewness for each numeric column\n",
    "skewness_df = df.select([F.skewness(F.col(c)).alias(f'{c}_skewness') for c in numeric_cols])\n",
    "\n",
    "# Step 2: Calculate kurtosis for each numeric column\n",
    "kurtosis_df = df.select([F.kurtosis(F.col(c)).alias(f'{c}_kurtosis') for c in numeric_cols])\n",
    "\n",
    "# Step 3: Collect the results into Pandas DataFrames\n",
    "skewness_values = skewness_df.collect()[0].asDict()\n",
    "kurtosis_values = kurtosis_df.collect()[0].asDict()\n",
    "\n",
    "skewness_pandas_df = pd.DataFrame(list(skewness_values.items()), columns=['Column Name', 'Skewness'])\n",
    "kurtosis_pandas_df = pd.DataFrame(list(kurtosis_values.items()), columns=['Column Name', 'Kurtosis'])\n",
    "\n",
    "# Step 4: Style the skewness and kurtosis tables\n",
    "styled_skewness = skewness_pandas_df.style.background_gradient(cmap=\"Blues\") \\\n",
    "                                        .set_table_styles([{'selector': 'thead th', \n",
    "                                                            'props': [('background-color', '#1E90FF'), \n",
    "                                                                      ('color', 'white')]}]) \\\n",
    "                                        .set_properties(**{'text-align': 'center'}) \\\n",
    "                                        .set_caption(\"Skewness for Numeric Columns\")\n",
    "\n",
    "styled_kurtosis = kurtosis_pandas_df.style.background_gradient(cmap=\"Oranges\") \\\n",
    "                                      .set_table_styles([{'selector': 'thead th', \n",
    "                                                          'props': [('background-color', '#FF4500'), \n",
    "                                                                    ('color', 'white')]}]) \\\n",
    "                                      .set_properties(**{'text-align': 'center'}) \\\n",
    "                                      .set_caption(\"Kurtosis for Numeric Columns\")\n",
    "\n",
    "# Step 5: Display the styled tables (works in Jupyter notebook)\n",
    "display(styled_skewness)\n",
    "display(styled_kurtosis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outliers Detection using Z-Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List of numeric columns\n",
    "numeric_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n",
    "# Initialize the original DataFrame with Z-Score columns\n",
    "zscore_df = df\n",
    "\n",
    "# Calculate Z-Score for each numeric column and add it to the DataFrame\n",
    "for col_name in numeric_cols:\n",
    "    stats = df.select(F.mean(F.col(col_name)).alias('mean'), F.stddev(F.col(col_name)).alias('std')).first()\n",
    "    mean_value, std_value = stats['mean'], stats['std']\n",
    "    \n",
    "    zscore_df = zscore_df.withColumn(f'{col_name}_zscore', (F.col(col_name) - mean_value) / std_value)\n",
    "\n",
    "# Convert to Pandas DataFrame for better formatting\n",
    "pandas_zscore_df = zscore_df.select(*numeric_cols, *[f'{col}_zscore' for col in numeric_cols]).toPandas()\n",
    "\n",
    "# Set display options for better formatting\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Don't wrap to next line\n",
    "\n",
    "# Print the top 20 rows of the Z-Scores DataFrame\n",
    "print(\"Top 20 rows of the Z-Scores DataFrame:\")\n",
    "# print(pandas_zscore_df.head(20))\n",
    "\n",
    "# Display the styled DataFrame for the first 20 rows\n",
    "styled_zscore = pandas_zscore_df.head(20).style.background_gradient(cmap=\"viridis\") \\\n",
    "                                        .set_table_styles([{'selector': 'thead th', \n",
    "                                                            'props': [('background-color', '#4B0082'), \n",
    "                                                                      ('color', 'white')]}]) \\\n",
    "                                        .set_properties(**{'text-align': 'center'}) \\\n",
    "                                        .set_caption(\"Z-Scores for Numeric Columns (Top 20 Rows)\")\n",
    "\n",
    "# Display the styled DataFrame in a Jupyter Notebook\n",
    "styled_zscore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Quantile Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quantiles (0%, 25%, 50%, 75%, and 100%)\n",
    "quantiles = df.approxQuantile(numeric_cols, [0.0, 0.25, 0.5, 0.75, 1.0], 0.05)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "quantiles_df = pd.DataFrame(quantiles, columns=[\"0%\", \"25%\", \"50%\", \"75%\", \"100%\"], index=numeric_cols)\n",
    "\n",
    "# Style the DataFrame\n",
    "styled_quantiles = quantiles_df.style \\\n",
    "    .background_gradient(cmap=\"viridis\") \\\n",
    "    .set_table_styles([{'selector': 'thead th', \n",
    "                        'props': [('background-color', '#4B0082'), \n",
    "                                  ('color', 'white')]}]) \\\n",
    "    .set_properties(**{'text-align': 'center'}) \\\n",
    "    .set_caption(\"Quantiles for Numeric Columns\")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_quantiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'numeric_cols' contains the list of numeric columns\n",
    "covariance_results = []\n",
    "\n",
    "# Calculate covariance between pairs of numeric columns\n",
    "for col1 in numeric_cols:\n",
    "    for col2 in numeric_cols:\n",
    "        if col1 != col2:\n",
    "            cov_value = df.stat.cov(col1, col2)  # Use stat.cov for covariance in PySpark\n",
    "            covariance_results.append([col1, col2, cov_value])\n",
    "\n",
    "# Randomly sample 3 to 5 rows from the covariance results\n",
    "random_sample = random.sample(covariance_results, min(len(covariance_results), random.randint(3, 5)))\n",
    "\n",
    "# Convert the random sample to a Pandas DataFrame for styling (PySpark doesn't support advanced styling directly)\n",
    "covariance_df = pd.DataFrame(random_sample, columns=[\"Column 1\", \"Column 2\", \"Covariance\"])\n",
    "\n",
    "# Style the DataFrame using Pandas\n",
    "styled_covariance = covariance_df.style \\\n",
    "    .background_gradient(cmap=\"viridis\") \\\n",
    "    .set_table_styles([{'selector': 'thead th', \n",
    "                        'props': [('background-color', '#4B0082'), \n",
    "                                  ('color', 'white')]}]) \\\n",
    "    .set_properties(**{'text-align': 'center'}) \\\n",
    "    .set_caption(\"Sampled Covariance Between Numeric Columns\")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_covariance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ratio of Variance (F-test for Feature Importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of numeric columns\n",
    "numeric_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', \n",
    "                'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n",
    "\n",
    "# Calculate variance for each numeric column\n",
    "variance_results = []\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    variance = df.select(F.variance(F.col(col_name))).collect()[0][0]\n",
    "    variance_results.append([col_name, variance])\n",
    "\n",
    "# Convert the results to a Pandas DataFrame for visualization\n",
    "variance_df = pd.DataFrame(variance_results, columns=[\"Column Name\", \"Variance\"])\n",
    "\n",
    "# Style the DataFrame\n",
    "styled_variance = variance_df.style \\\n",
    "    .background_gradient(cmap=\"viridis\") \\\n",
    "    .set_table_styles([{'selector': 'thead th', \n",
    "                        'props': [('background-color', '#4B0082'), \n",
    "                                  ('color', 'white')]}]) \\\n",
    "    .set_properties(**{'text-align': 'center'}) \\\n",
    "    .set_caption(\"Variance of Numeric Columns\")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visvalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot distribution for each numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas DataFrame for visualization\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)  # Slightly larger font scale for better readability\n",
    "sns.set_palette(\"Set2\")  # Use Set2 color palette for a soft, professional look\n",
    "\n",
    "# Define an enhanced function to plot the distribution with more customization\n",
    "def plot_distribution(col_name):\n",
    "    plt.figure(figsize=(12, 6))  # Increase figure size for better visibility\n",
    "    sns.histplot(pandas_df[col_name], kde=True, color='dodgerblue', bins=30, stat='density', alpha=0.7)  # Updated color and transparency\n",
    "    plt.axvline(pandas_df[col_name].mean(), color='red', linestyle='--', label=f'Mean: {pandas_df[col_name].mean():.2f}')  # Mean line with label\n",
    "    plt.axvline(pandas_df[col_name].median(), color='green', linestyle='-', label=f'Median: {pandas_df[col_name].median():.2f}')  # Median line with label\n",
    "    \n",
    "    # Enhancements\n",
    "    plt.title(f'Distribution of {col_name}', fontsize=20, fontweight='bold', color='navy')  # Title customization with color\n",
    "    plt.xlabel(col_name, fontsize=14, labelpad=10, color='navy')  # X-axis label with padding\n",
    "    plt.ylabel('Density', fontsize=14, labelpad=10, color='navy')  # Y-axis label with padding\n",
    "    plt.xticks(fontsize=12, rotation=45, color='darkblue')  # X-axis tick labels with rotation\n",
    "    plt.yticks(fontsize=12, color='darkblue')  # Y-axis tick labels\n",
    "    plt.legend(fontsize=12, loc='upper right', frameon=True, shadow=True)  # Add a shadowed legend for better visibility\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)  # Add grid with transparency and dashed style\n",
    "    plt.tight_layout()  # Adjust layout to fit labels\n",
    "    \n",
    "    # Additional styling\n",
    "    plt.gca().spines['top'].set_visible(False)  # Remove the top spine\n",
    "    plt.gca().spines['right'].set_visible(False)  # Remove the right spine\n",
    "    plt.gca().spines['left'].set_color('grey')  # Color the left spine\n",
    "    plt.gca().spines['bottom'].set_color('grey')  # Color the bottom spine\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot distribution for each numeric column using the enhanced function\n",
    "for col_name in numeric_cols:\n",
    "    plot_distribution(col_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box plot for Numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'numeric_cols' is a list of your numeric columns\n",
    "# Convert the PySpark DataFrame to Pandas for plotting\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)  # Slightly larger font scale for better readability\n",
    "sns.set_palette(\"Set2\")  # Use Set2 color palette for soft, professional tones\n",
    "\n",
    "# Define a function to create enhanced box plots\n",
    "def plot_boxplot(col_name):\n",
    "    plt.figure(figsize=(10, 6))  # Increase figure size for better visibility\n",
    "    sns.boxplot(x=pandas_df[col_name], color='#6A5ACD', width=0.4, fliersize=5, linewidth=1.5)  # Box width and outlier size\n",
    "    plt.title(f'Box Plot of {col_name}', fontsize=20, fontweight='bold', color='#333')  # Enhanced title styling\n",
    "    plt.xlabel(col_name, fontsize=16, color='#555')  # X-axis label with custom color\n",
    "    plt.xticks(fontsize=14, color='#333')  # X-axis tick labels\n",
    "    plt.grid(True, linestyle='--', linewidth=0.6, alpha=0.7)  # Add dashed grid with transparency for better readability\n",
    "    \n",
    "    # Add median line with enhanced design\n",
    "    median_value = pandas_df[col_name].median()\n",
    "    plt.axvline(x=median_value, color='orange', linestyle='--', label=f'Median: {median_value:.2f}')  # Display median\n",
    "    \n",
    "    # Annotate the median value on the plot\n",
    "    plt.annotate(f'Median: {median_value:.2f}', \n",
    "                 xy=(median_value, 0.05), \n",
    "                 xytext=(median_value + 0.5, 0.2),  # Adjust annotation position\n",
    "                 arrowprops=dict(facecolor='orange', shrink=0.05),\n",
    "                 fontsize=12, color='darkorange')\n",
    "\n",
    "    plt.legend(fontsize=12, loc='best', frameon=True, shadow=True)  # Add legend for median line\n",
    "    plt.gca().spines['top'].set_visible(False)  # Remove the top spine for cleaner look\n",
    "    plt.gca().spines['right'].set_visible(False)  # Remove the right spine\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to fit labels and avoid overlapping\n",
    "    plt.show()\n",
    "\n",
    "# Create box plots for each numeric column using the enhanced function\n",
    "for col_name in numeric_cols:\n",
    "    plot_boxplot(col_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your PySpark DataFrame and 'numeric_cols' is a list of numeric columns\n",
    "\n",
    "# Convert the PySpark DataFrame to Pandas for visualization\n",
    "pandas_df = df.select(numeric_cols).toPandas()\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)  # Slightly larger font scale for better readability\n",
    "\n",
    "# Create a pair plot with enhanced appearance\n",
    "pair_plot = sns.pairplot(\n",
    "    pandas_df[numeric_cols], \n",
    "    diag_kind='kde', \n",
    "    palette=\"coolwarm\",  # Using a cool-warm palette for better contrast\n",
    "    height=3  # Set height for each subplot\n",
    ")\n",
    "\n",
    "# Add titles and adjust aesthetics\n",
    "pair_plot.fig.suptitle(\"Enhanced Pair Plot of Numeric Columns\", fontsize=22, fontweight='bold', y=1.03)  # Add a title\n",
    "\n",
    "# Customize the lower triangle with scatter plots\n",
    "pair_plot.map_lower(sns.scatterplot, color='teal', alpha=0.7, edgecolor=\"black\")  # Add border for points\n",
    "\n",
    "# Add correlation values in the upper triangle for more information\n",
    "def annotate_correlation(x, y, **kwargs):\n",
    "    corr = x.corr(y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(f'Corr: {corr:.2f}', xy=(0.5, 0.5), xycoords=ax.transAxes, \n",
    "                ha='center', va='center', fontsize=12, color='darkred')\n",
    "\n",
    "pair_plot.map_upper(annotate_correlation)\n",
    "\n",
    "# Customize diagonal with KDE plots\n",
    "pair_plot.map_diag(sns.kdeplot, color='darkblue', lw=2, shade=True)  # KDE with shade and thicker line\n",
    "\n",
    "# Adjust the layout to fit everything nicely\n",
    "plt.subplots_adjust(top=0.92, wspace=0.2, hspace=0.2)  # Adjust margins to fit title and subplots\n",
    "\n",
    "# Show the final plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Columns Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Density Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'df' is your PySpark DataFrame\n",
    "# Numeric columns based on your dataset\n",
    "numeric_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', \n",
    "                'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', \n",
    "                'absences', 'G1', 'G2', 'G3']\n",
    "\n",
    "# Extract relevant columns from PySpark DataFrame\n",
    "heatmap_df = df.select(numeric_cols)\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "pandas_heatmap_df = heatmap_df.toPandas()\n",
    "\n",
    "# Set up the figure with subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  # Adjusted layout to fit more plots\n",
    "\n",
    "# 1. Density Heatmap for Age vs G1 Score\n",
    "sns.kdeplot(data=pandas_heatmap_df, x='age', y='G1', cmap='Blues', fill=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Density Heatmap: Age vs G1 Score', fontsize=16)\n",
    "axes[0, 0].set_xlabel('Age', fontsize=14)\n",
    "axes[0, 0].set_ylabel('G1 Score', fontsize=14)\n",
    "\n",
    "# 2. Density Heatmap for Medu vs G2 Score\n",
    "sns.kdeplot(data=pandas_heatmap_df, x='Medu', y='G2', cmap='Greens', fill=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Density Heatmap: Mother\\'s Education vs G2 Score', fontsize=16)\n",
    "axes[0, 1].set_xlabel('Mother\\'s Education (Medu)', fontsize=14)\n",
    "axes[0, 1].set_ylabel('G2 Score', fontsize=14)\n",
    "\n",
    "# 3. Density Heatmap for Study Time vs G3 Score\n",
    "sns.kdeplot(data=pandas_heatmap_df, x='studytime', y='G3', cmap='Reds', fill=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Density Heatmap: Study Time vs G3 Score', fontsize=16)\n",
    "axes[1, 0].set_xlabel('Study Time', fontsize=14)\n",
    "axes[1, 0].set_ylabel('G3 Score', fontsize=14)\n",
    "\n",
    "# 4. Density Heatmap for Failures vs G1 Score\n",
    "sns.kdeplot(data=pandas_heatmap_df, x='failures', y='G1', cmap='Purples', fill=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Density Heatmap: Failures vs G1 Score', fontsize=16)\n",
    "axes[1, 1].set_xlabel('Failures', fontsize=14)\n",
    "axes[1, 1].set_ylabel('G1 Score', fontsize=14)\n",
    "\n",
    "# 5. Density Heatmap for Absences vs G2 Score\n",
    "sns.kdeplot(data=pandas_heatmap_df, x='absences', y='G2', cmap='Oranges', fill=True, ax=axes[2, 0])\n",
    "axes[2, 0].set_title('Density Heatmap: Absences vs G2 Score', fontsize=16)\n",
    "axes[2, 0].set_xlabel('Absences', fontsize=14)\n",
    "axes[2, 0].set_ylabel('G2 Score', fontsize=14)\n",
    "\n",
    "# 6. Density Heatmap for Freetime vs G3 Score\n",
    "sns.kdeplot(data=pandas_heatmap_df, x='freetime', y='G3', cmap='Blues', fill=True, ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Density Heatmap: Free Time vs G3 Score', fontsize=16)\n",
    "axes[2, 1].set_xlabel('Free Time', fontsize=14)\n",
    "axes[2, 1].set_ylabel('G3 Score', fontsize=14)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Student's Sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "df.createOrReplaceTempView(\"students\")\n",
    "# Count female students\n",
    "female_count = spark.sql(\"SELECT COUNT(*) AS female_count FROM students WHERE sex = 'F'\")\n",
    "print('Number of female students:', female_count.collect()[0]['female_count'])\n",
    "\n",
    "# Count male students\n",
    "male_count = spark.sql(\"SELECT COUNT(*) AS male_count FROM students WHERE sex = 'M'\")\n",
    "print('Number of male students:', male_count.collect()[0]['male_count'])\n",
    "# Count male and female students\n",
    "gender_count = spark.sql(\"SELECT sex, COUNT(*) AS count FROM students GROUP BY sex\")\n",
    "gender_count_pd = gender_count.toPandas()  # Convert to Pandas DataFrame\n",
    "\n",
    "# Set the style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create a countplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='sex', data=gender_count_pd, palette='plasma')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Count of Male and Female Students', fontsize=16)\n",
    "plt.xlabel('Sex', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: The gender distribution is pretty even.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Age of Students\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the age column and convert to Pandas DataFrame\n",
    "age_data = df.select(\"age\").toPandas()\n",
    "\n",
    "# Set the style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create a KDE plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "b = sns.kdeplot(age_data['age'], fill=True)\n",
    "\n",
    "# Set titles and labels\n",
    "b.axes.set_title('Ages of Students', fontsize=16)\n",
    "b.set_xlabel('Age', fontsize=14)\n",
    "b.set_ylabel('Density', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: The student age seems to be ranging from 15-19, where gender distribution is pretty even in each age group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Students from Urban & Rural Areas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "df.createOrReplaceTempView(\"students\")\n",
    "\n",
    "# df Urban students\n",
    "urban_count = spark.sql(\"SELECT COUNT(*) AS urban_count FROM students WHERE address = 'U'\")\n",
    "u_stud = urban_count.collect()[0]['urban_count']\n",
    "print('Number of Urban students:', u_stud)\n",
    "\n",
    "# Count Rural students\n",
    "rural_count = spark.sql(\"SELECT COUNT(*) AS rural_count FROM students WHERE address = 'R'\")\n",
    "r_stud = rural_count.collect()[0]['rural_count']\n",
    "print('Number of Rural students:', r_stud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Address Count Plot\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Group by 'address' and count occurrences\n",
    "address_counts = df.groupBy('address').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "address_counts_pd = address_counts.toPandas()\n",
    "\n",
    "# Get colors from the 'magma' colormap\n",
    "colors = cm.get_cmap('magma', len(address_counts_pd))  # Create a colormap\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(address_counts_pd['address'], address_counts_pd['count'], color=colors(range(len(address_counts_pd))))\n",
    "plt.title('Number of Students by Address')\n",
    "plt.xlabel('Address')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obervations: Approximately 77.72% students come from urban region and 22.28% from rural region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Address G3 Count Plot\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Group by 'address' and 'G3' and count occurrences\n",
    "address_g3_counts = df.groupBy('address', 'G3').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "address_g3_counts_pd = address_g3_counts.toPandas()\n",
    "\n",
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Color palette for better visual distinction\n",
    "palette = sns.color_palette(\"magma\", len(address_g3_counts_pd['G3'].unique()))\n",
    "\n",
    "# Plot bars for each G3 value\n",
    "for idx, g3_value in enumerate(address_g3_counts_pd['G3'].unique()):\n",
    "    subset = address_g3_counts_pd[address_g3_counts_pd['G3'] == g3_value]\n",
    "    plt.bar(subset['address'], subset['count'], label=f'G3 = {g3_value}', alpha=0.7, color=palette[idx])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Number of Students by Address and G3', fontsize=18, weight='bold')\n",
    "plt.xlabel('Address', fontsize=14)\n",
    "plt.ylabel('Count of Students', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels for better visibility\n",
    "plt.legend(title='G3 Values', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels on top of bars\n",
    "for idx, g3_value in enumerate(address_g3_counts_pd['G3'].unique()):\n",
    "    subset = address_g3_counts_pd[address_g3_counts_pd['G3'] == g3_value]\n",
    "    for i in range(len(subset)):\n",
    "        plt.text(x=subset['address'].iloc[i], \n",
    "                 y=subset['count'].iloc[i] + 1,  # Slightly above the bar\n",
    "                 s=subset['count'].iloc[i],\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust the layout to prevent clipping\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Does age affect final grade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Age vs Final Grade Boxplot\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame, convert it to Pandas DataFrame\n",
    "df_pd = df.toPandas()  # Convert PySpark DataFrame to Pandas DataFrame\n",
    "\n",
    "# Set the seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "b = sns.boxplot(x='age', y='G3', data=df_pd, palette='gist_heat')\n",
    "\n",
    "# Set title and labels\n",
    "b.set_title('Age vs Final Grade', fontsize=18, weight='bold')\n",
    "b.set_xlabel('Age', fontsize=14)\n",
    "b.set_ylabel('Final Grade (G3)', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: \n",
    "* Plotting the distribution rather than statistics would help us better understand the data.\n",
    "* The above plot shows that the median grades of the three age groups(15,16,17) are similar. Note the skewness of age group 19. (may be due to sample size). Age group 20 seems to score highest grades among all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Age vs Final Grade Swarm Plot\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame, convert it to Pandas DataFrame\n",
    "df_pd = df.toPandas()  # Convert PySpark DataFrame to Pandas DataFrame\n",
    "\n",
    "# Set the seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the swarm plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "b = sns.swarmplot(x='age', y='G3', hue='sex', data=df_pd, palette='PiYG', dodge=True)\n",
    "\n",
    "# Set title and labels\n",
    "b.set_title('Does Age Affect Final Grade?', fontsize=18, weight='bold')\n",
    "b.set_xlabel('Age', fontsize=14)\n",
    "b.set_ylabel('Final Grade (G3)', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Do urban students perform better than rural students?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Grade Distribution by Address\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming 'stud' is your PySpark DataFrame\n",
    "# Calculate grades for Urban and Rural students\n",
    "urban_grades = df.filter(df['address'] == 'U').select('G3').toPandas()\n",
    "rural_grades = df.filter(df['address'] == 'R').select('G3').toPandas()\n",
    "\n",
    "# Set the seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the KDE plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(urban_grades['G3'], label='Urban', shade=True)\n",
    "sns.kdeplot(rural_grades['G3'], label='Rural', shade=True)\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Do Urban Students Score Higher Than Rural Students?', fontsize=18)\n",
    "plt.xlabel('Grade', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: The above graph clearly shows there is not much difference between the grades based on location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Student status by alchool consumption :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weekend Alcohol Consumption Impact\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming 'stud' is your PySpark DataFrame\n",
    "# Create a crosstab\n",
    "alc_tab = df.groupBy('G3', 'Walc').count()\n",
    "\n",
    "# Calculate total counts for each 'G3' status\n",
    "total_counts = alc_tab.groupBy('G3').agg(F.sum('count').alias('total_count'))\n",
    "\n",
    "# Join to get the percentages\n",
    "alc_perc = alc_tab.join(total_counts, on='G3') \\\n",
    "    .withColumn('percentage', (alc_tab['count'] / total_counts['total_count']) * 100) \\\n",
    "    .select('G3', 'Walc', 'percentage')\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "alc_perc_pd = alc_perc.toPandas()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "alc_perc_pd.pivot(index='G3', columns='Walc', values='percentage').plot(kind='bar', colormap=\"Dark2_r\", ax=plt.gca(), fontsize=16)\n",
    "\n",
    "plt.title('Student Status by Weekend Alcohol Consumption', fontsize=20)\n",
    "plt.xlabel('Student Status', fontsize=16)\n",
    "plt.ylabel('Percentage of Students', fontsize=16)\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better visibility\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: We come to know students between range 40% - 45% consume alcohol weekly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Student status by internet accessibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Internet Accessibility Impact\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming 'stud' is your PySpark DataFrame\n",
    "# Create a crosstab\n",
    "alc_tab = df.groupBy('G3', 'internet').count()\n",
    "\n",
    "# Calculate total counts for each 'G3' status\n",
    "total_counts = alc_tab.groupBy('G3').agg(F.sum('count').alias('total_count'))\n",
    "\n",
    "# Join to get the percentages\n",
    "alc_perc = alc_tab.join(total_counts, on='G3') \\\n",
    "    .withColumn('percentage', (alc_tab['count'] / total_counts['total_count']) * 100) \\\n",
    "    .select('G3', 'internet', 'percentage')\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "alc_perc_pd = alc_perc.toPandas()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "alc_perc_pd.pivot(index='G3', columns='internet', values='percentage').plot(kind='bar', colormap=\"Dark2_r\", ax=plt.gca(), fontsize=16)\n",
    "\n",
    "plt.title('Student Status by Internet Accessibility', fontsize=20)\n",
    "plt.xlabel('Student Status', fontsize=16)\n",
    "plt.ylabel('Percentage of Students', fontsize=16)\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better visibility\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Majority of the students have internet connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Going Out with Friends Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Go Out vs Final Grade\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming 'stud' is your PySpark DataFrame\n",
    "# Group by 'goout' and count occurrences\n",
    "goout_counts = df.groupBy('goout').agg(F.count('*').alias('count'))\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "goout_counts_pd = goout_counts.toPandas()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(goout_counts_pd['goout'], goout_counts_pd['count'], color='orangered', alpha=0.7)\n",
    "\n",
    "# Setting the title and labels\n",
    "plt.title('Go Out vs Final Grade (G3)', fontsize=20)\n",
    "plt.xlabel('Go Out Frequency', fontsize=16)\n",
    "plt.ylabel('Count of Students', fontsize=16)\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better visibility\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : The students have an average score when it comes to going out with friends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* numeric_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', \n",
    "* 'failures', 'famrel', 'freetime', 'goout', \n",
    "* 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Conversion of numerical columns to feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ML_Model_Implementation\").getOrCreate()\n",
    "\n",
    "# Load your dataset\n",
    "new = spark.read.csv(\"dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert numerical columns to features vector\n",
    "numerical_columns = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', \n",
    "                     'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n",
    "assembler = VectorAssembler(inputCols=numerical_columns[:-3], outputCol=\"features\")\n",
    "\n",
    "# Transform the dataset and create the label column\n",
    "new = assembler.transform(new).withColumn(\"label\", (col(\"G3\") > 10).cast(\"int\"))\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data = new.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Logistic Regression Model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluating Logistic Regression\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "\n",
    "# Training Accuracy for Logistic Regression\n",
    "train_predictions_lr = lr_model.transform(train_data)\n",
    "train_accuracy_lr = evaluator.evaluate(train_predictions_lr)\n",
    "\n",
    "# Confusion Matrix and Classification Report for Logistic Regression\n",
    "def compute_confusion_matrix_and_report(predictions):\n",
    "    predictions_and_labels = predictions.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "    # Extract predicted and actual labels\n",
    "    y_pred = np.array(predictions_and_labels.map(lambda x: x[0]).collect())\n",
    "    y_true = np.array(predictions_and_labels.map(lambda x: x[1]).collect())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Classification Report\n",
    "    class_report = classification_report(y_true, y_pred)\n",
    "\n",
    "    return conf_matrix, class_report\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "conf_matrix_lr, class_report_lr = compute_confusion_matrix_and_report(lr_predictions)\n",
    "\n",
    "# Print results\n",
    "print(f\"Logistic Regression Training Accuracy: {train_accuracy_lr}\")\n",
    "print(f\"Logistic Regression Testing Accuracy: {lr_accuracy}\")\n",
    "print(\"Confusion Matrix for Logistic Regression:\\n\", conf_matrix_lr)\n",
    "print(\"Classification Report for Logistic Regression:\\n\", class_report_lr)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(conf_matrix, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for Logistic Regression\n",
    "plot_confusion_matrix(conf_matrix_lr, \"Logistic Regression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2. Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluating Random Forest\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "\n",
    "# Training Accuracy for Random Forest\n",
    "train_predictions_rf = rf_model.transform(train_data)\n",
    "train_accuracy_rf = evaluator.evaluate(train_predictions_rf)\n",
    "\n",
    "# Confusion Matrix and Classification Report for Random Forest\n",
    "def compute_confusion_matrix_and_report(predictions):\n",
    "    predictions_and_labels = predictions.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "    # Extract predicted and actual labels\n",
    "    y_pred = np.array(predictions_and_labels.map(lambda x: x[0]).collect())\n",
    "    y_true = np.array(predictions_and_labels.map(lambda x: x[1]).collect())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Classification Report\n",
    "    class_report = classification_report(y_true, y_pred)\n",
    "\n",
    "    return conf_matrix, class_report\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "conf_matrix_rf, class_report_rf = compute_confusion_matrix_and_report(rf_predictions)\n",
    "\n",
    "# Print results\n",
    "print(f\"Random Forest Training Accuracy: {train_accuracy_rf}\")\n",
    "print(f\"Random Forest Testing Accuracy: {rf_accuracy}\")\n",
    "print(\"Confusion Matrix for Random Forest:\\n\", conf_matrix_rf)\n",
    "print(\"Classification Report for Random Forest:\\n\", class_report_rf)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(conf_matrix, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for Random Forest\n",
    "plot_confusion_matrix(conf_matrix_rf, \"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. Gradient Boosting Classifier\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Evaluating Gradient Boosting\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "# Training Accuracy for Gradient Boosting\n",
    "train_predictions_gbt = gbt_model.transform(train_data)\n",
    "train_accuracy_gbt = evaluator.evaluate(train_predictions_gbt)\n",
    "\n",
    "# Confusion Matrix and Classification Report for Gradient Boosting\n",
    "def compute_confusion_matrix_and_report(predictions):\n",
    "    predictions_and_labels = predictions.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "    # Extract predicted and actual labels\n",
    "    y_pred = np.array(predictions_and_labels.map(lambda x: x[0]).collect())\n",
    "    y_true = np.array(predictions_and_labels.map(lambda x: x[1]).collect())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Classification Report\n",
    "    class_report = classification_report(y_true, y_pred)\n",
    "\n",
    "    return conf_matrix, class_report\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "conf_matrix_gbt, class_report_gbt = compute_confusion_matrix_and_report(gbt_predictions)\n",
    "\n",
    "# Print results\n",
    "print(f\"Gradient Boosting Training Accuracy: {train_accuracy_gbt}\")\n",
    "print(f\"Gradient Boosting Testing Accuracy: {gbt_accuracy}\")\n",
    "print(\"Confusion Matrix for Gradient Boosting:\\n\", conf_matrix_gbt)\n",
    "print(\"Classification Report for Gradient Boosting:\\n\", class_report_gbt)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(conf_matrix, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for Gradient Boosting\n",
    "plot_confusion_matrix(conf_matrix_gbt, \"Gradient Boosting\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### All Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all results at the end\n",
    "for res in results:\n",
    "    print(f\"Model: {res['model']}, Accuracy: {res['accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training and Validation Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store accuracies for each model\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "model_names = [\"Logistic Regression\", \"Random Forest\", \"Gradient Boosting\"]\n",
    "\n",
    "# Evaluator instance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Logistic Regression Training and Validation Accuracy\n",
    "train_predictions_lr = lr_model.transform(train_data)\n",
    "train_accuracy_lr = evaluator.evaluate(train_predictions_lr)\n",
    "validation_accuracy_lr = evaluator.evaluate(lr_predictions)\n",
    "\n",
    "train_accuracies.append(train_accuracy_lr)\n",
    "validation_accuracies.append(validation_accuracy_lr)\n",
    "\n",
    "# Random Forest Training and Validation Accuracy\n",
    "train_predictions_rf = rf_model.transform(train_data)\n",
    "train_accuracy_rf = evaluator.evaluate(train_predictions_rf)\n",
    "validation_accuracy_rf = evaluator.evaluate(rf_predictions)\n",
    "\n",
    "train_accuracies.append(train_accuracy_rf)\n",
    "validation_accuracies.append(validation_accuracy_rf)\n",
    "\n",
    "# Gradient Boosting Training and Validation Accuracy\n",
    "train_predictions_gbt = gbt_model.transform(train_data)\n",
    "train_accuracy_gbt = evaluator.evaluate(train_predictions_gbt)\n",
    "validation_accuracy_gbt = evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "train_accuracies.append(train_accuracy_gbt)\n",
    "validation_accuracies.append(validation_accuracy_gbt)\n",
    "\n",
    "# Plotting Training and Validation Accuracy for each model\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.plot(model_names, train_accuracies, label='Training Accuracy', marker='o')\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.plot(model_names, validation_accuracies, label='Validation Accuracy', marker='o')\n",
    "\n",
    "plt.title(\"Training and Validation Accuracy for ML Models\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)  # Accuracy ranges between 0 and 1\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Logistic Regression Training Accuracy: {train_accuracy_lr}\")\n",
    "print(f\"Logistic Regression Validation Accuracy: {validation_accuracy_lr}\")\n",
    "print(f\"Random Forest Training Accuracy: {train_accuracy_rf}\")\n",
    "print(f\"Random Forest Validation Accuracy: {validation_accuracy_rf}\")\n",
    "print(f\"Gradient Boosting Training Accuracy: {train_accuracy_gbt}\")\n",
    "print(f\"Gradient Boosting Validation Accuracy: {validation_accuracy_gbt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Define numerical columns\n",
    "numerical_columns = ['age', 'studytime', 'failures', \n",
    "                     'Medu', 'G1', 'G2']\n",
    "\n",
    "# Prepare the features and label\n",
    "X = data[numerical_columns]  # Use all 6 features (including G1 and G2)\n",
    "y = (data['G3'] > 10).astype(int)  # Label: binary classification based on G3\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features (optional but often beneficial)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the entire Random Forest model\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "print(\"Random Forest model saved to 'random_forest_model.pkl'\")\n",
    "\n",
    "# Loading the model for prediction later (if needed)\n",
    "with open('random_forest_model.pkl', 'rb') as f:\n",
    "    loaded_rf_model = pickle.load(f)\n",
    "\n",
    "print(\"Loaded model parameters:\", loaded_rf_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyper Tuning and Aditional Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Define numerical columns\n",
    "numerical_columns = ['age', 'studytime', 'failures', 'Medu', 'G1', 'G2']\n",
    "\n",
    "# Prepare the features and label\n",
    "X = data[numerical_columns]  # Use all 6 features (including G1 and G2)\n",
    "y = (data['G3'] > 10).astype(int)  # Label: binary classification based on G3\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']  # Removed 'auto'\n",
    "}\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from grid search\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Use the best model found by GridSearchCV\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Save the trained Random Forest model\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_rf_model, f)\n",
    "\n",
    "# Save the scaler for future use\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Random Forest model and scaler saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
